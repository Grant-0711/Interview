# 实时数仓

ods层：kafka 2个topic

​	日志数据

​		直接自定义生产者，发到kafka

​			理由：？

​	业务数据

​		maxwell或者 cdc ：需要自定义反序列化器

dwd（kafka）+dim层（hbase）

​	why hbase 存 维度数据？

​		长远考虑，如果用mysql存的话随着数据量的上升会出现瓶颈

​	dwd

​		日志：侧流输出，分成3条，目前有两条未使用，所以3个topic

​		业务：动态分流

​			mysql中定义配置表，决定每张表的去向（维度表和事实表）	

​			cdc sql 读取配置表  制作成 广播状态

​	redis旁路缓存：加速查询Hbase

​		保证缓存一致性

​			如果维度数据发生变化，更新hbase同时将redis的缓存删掉

​			redis过期时间为24小时

​	业务表：kafka里面有十几张，hbase十几张维度表

dwm层 kafka

构造dws宽表时要多次重复用到dwd和dim层的表

因此为了实时场景减少复用，做一个宽表的过渡层

事实表之间用interval join

事实表与维度表 用异步io+旁路缓存



dws层：clickhouse

轻度聚合，减少维度表的查询，减少clickhouse的写入次数

轻度聚合，减少维度表查询，减少clickhouse的写入次数

实现：开一个小窗口，10s（5s）



ads层/可视化（最终的聚合）:

sugar 通过jdbc 查询 clickhouse



为什么宽表放在clickhouse：

极快，非常适合查询分析，聚合

不适合join，多表join效率差

clickhouse适合 宽表这种，字段非常多，每次查询只需要用到几个列，对其做聚合但是不做join的场景



为什么维度表放hbase

长远考虑，考虑数据增量会有瓶颈



clickhouse支不支持更新？

不支持事务更新，删除操作

通过 alter table 实现 更新，删除的效果  to  分片的时候才去执行更新，删除





各层kafka加起来总共20多个topic





# 实时项目串讲

采集路线，数据分两大类，用户行为日志数据和业务数据

其中日志数据我们从日志服务器自定义生产者发送到kafka

是否展开kafka 相关？

业务数据是存在mysql的。我们在制定框架解决方案时考虑了两种同步增量数据的策略，并且都进行了实验，一是使用flink自带的cdc，二是使用maxwell

是否展开cdc？是否展开maxwell？



同步到kafka的原始数据也就是ods层数据，有两个topic，是日志数据topic和业务数据topic

我们在ods层之后添加了dwd层和dim层，dwd层的主要作用是对数据分流，详细来说，

针对日志数据，总共有五类是启动，页面，行为，曝光，错误。在我们目前的项目架构中提取了其中的启动，页面，曝光和行为四种日志数据，分别写入kafka对应topic，形成dwd层日志数据

针对业务数据，我们采用动态分流。

详细来说，我们在mysql中创建了一张配置表，其中存放了所有涉及到的业务表名以及其对应的去处，去处分两个目的地，kafka和hbase。具体的，维度数据存在hbase，事实表数据存在kafka。维度表数据存在hbase的原因是一般变化少，长远来考虑数据量增大的情况的话，选hbase比较合适。

hbase是否展开？

动态分流的具体实现是

使用cdc读取mysql配置表信息做成广播流，将其与ods层消费到的业务数据流connect，根据业务数据流中数据的类型在广播流中获取对应的目的地，然后写出，我们是吧去向kafka的数据写入主流，去向hbase的数据写入测输出流，最后将其对应写入目的地



然后来到dwm层，由于构造dws宽表时要多次重复用到dwd和dim层的表

因此为了实时场景减少复用，做一个宽表的过渡层，这是dwm层存在的意义。这里根据业务需求我们初步制作了不同业务类别的宽表来对不同类别的业务进行分析，具体有流量统计，代表性的是UV和PV，用户行为，代表性的是跳出，订单相关，代表性的是订单宽表，商品相关，代表性的是商品宽表

这里举例订单宽表的实现思路，订单宽表涉及维度表和事实表的join。采用的是interval join

flink join 方式是否展开？

具体的，首先从hbase读维度数据，由于每次读DIM数据都连接Phoenix，这样效率不高

可以把用到的维度信息存储进内存，提升查询速度，先从缓存读，缓存有直接使用，缓存没有再去查询数据库，我们添加了redis旁路缓存：

redis是否展开？

同时，由于涉及到维度数据和事实数据的join，所以每来一条事实数据都要去join维度数据查询维度数据，这样连接Redis和Phoenix都需要网络io和传输时延，这样效率不高



考虑到flink1.2之后支持异步api，所以我们采用了异步io

具体的，使用Flink提供的AsyncDataStream

​	调用其unorderedWait方法，在重写的asyncInvoke方法中使用线程来完成异步请求

​	调用线程池的execute方法，在重写的run方法中实现业务：查询Dim数据，补齐维度



dwm层表完成后就可以进行dws层表的构建，dws层表是存在clickhouse的

dws层主要是根据某个维度主题将多个事实表数据进行轻度聚合（窗口级别），形成主题宽表，这样减少维度表查询，减少clickhouse的写入次数

clickhouse是否展开？

这一层我们目前主要针对的主题是搜索关键词，商品相关，访客相关，地区相关，零售店相关



这里举例搜索关键词和商品相关指标的实现

搜索关键词数据主要来源是日志数据，需要使用flink sql 建立动态表，与对应dwd层topic关联，获取数据，需要注意的是由于涉及开窗，要用事件时间，因此要在构建动态表时设置水印



获取数据中的搜索关键词数据：来自于page字段itemId，pageId是goodList

得到的数据需要自定义udtf函数进行切词

自定义udtf函数需要继承tableFunction，重载eval方法，要借用IKutil切词工具类，结果collect写出

然后再sql中内连接 lateral table 传入搜索词字段，对其结果分组开窗聚合，使用的是滚动窗口 tumble（et，interval  '5' second）



商品相关的指标包括点赞，收藏，加购物车，下单，支付，支付渠道，退款，退款商品topN，退款地区topN，商品分地区销量topN，商品厂家top1等

具体实现是读取相关topic数据到不同流，将其union，对union的综合流开窗聚合，然后补齐相应的维度数据，将结果写入clickhouse



# 迟到数据侧输出流输出，何时处理？定时器展开？

# topN问题？

# flink版本和CH版本的问题