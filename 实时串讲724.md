现在我先介绍一下之前做的实时项目

数据来源分为两大类，日志数据和业务数据，日志数据来源是日志服务器自定义kafka生产者发送到kafka，业务数据主要是mysql的业务表数据，大概三十多张表

关于业务数据的同步方式我们当时也是进行了调研选择了几个方案，例如flink自带的cdc，sqoop和maxwell等，初期调研的比较多的是flinkcdc，结果发现如果用flink sql的方式读的话只能单张表单张表的读，如果用流式api读的话拿到的数据除了数据本事还有一些其他讯息例如操作的类型等，需要自定义反序列化器来拿到自己想要的数据，比较麻烦，而使用maxwell的话相对而言方便些，主要配置文件中指定要读的库，就可以监控该库下表的增量变化。所以最后是用了maxwell。

完成日志数据和业务数据的同步之后数据都到了kafka，是直接日志数据一个topic，业务数据一个topic。所以要进行分流，那么现在进进入我们架构的dwd层

dwd层主要做的事情是数据的分流，具体来说针对日志数据，消费kafka 的数据，做成流之后，调用process算子进行分流，日志数据分五类，启动页面曝光行为错误，我们是目前业务使用的话除了错误别的四种都读进来了，启动放主流，其他放测输出流，然后分别把流写入到不同的kafka topic中

对于业务数据，考虑到在一个topic中有事实数据和维度数据，而两种数据的使用频率和场景不同，我们是考虑把维度数据存到hbase中形成dim层数据，这里使用hbase的原因是考虑到hbase的海量存储和读写速度快，可以解决mysql到后期如果业务数据量增大存储瓶颈的一个问题。而现在就有一个问题是kafka中的数据来一条我需要确定他是要存到h还是存到k去，如果在flink中写逻辑进行判断的话就要在flink中事先存好表的对应信息，如果设计到业务变动的话还要改代码，不是很灵活，所以我们考虑一个灵活的方案，在mysql中定义了一张配置表来指定表的去向，我们在flink中使用cdc去读这张配置表的信息，这里之所以用cdc也是之前提到用flinksql的形式读其实比较轻量简单，而现在的场景也只有一张表，正好适用，把读到的配置表信息广播出去。



这里要展开说一下维度表的读写详情，之前也说到维度表是存在hbase中的，如果要每来一条数据就使用phoenix建立连接的话效率会很低，网络io造成的延迟也会增大，所以我们这里也是做了优化，使用了redis做了一个旁路缓存，举例来讲是要读维度数据的场景会先去缓存读，如果缓存没有去hbase读，读完会把读到的数据也写进缓存，这样如果下次再读的话直接查缓存就可以，这里也考虑了维度数据的更新问题，如果维度数据有更新是也对缓存数据进行更新的。

刚才提到读维度数据要建立phoenix连接，所以也进行了另一个优化就是异步io的优化，flink在1.2之后开始支持异步io。我们也是用自定义线程池的方式在线程池中维持了phoenix连接，这样不用每次读维度数据都建立hbase连接

然后是到了dwm层这一层主要做的是一些复用指标的预处理和宽表的制作例如订单宽表和支付宽表，复用的指标例如uv和跳出

uv的实现思路主要是针对不同的mid，给一个状态，第一次来的时候判断状态是为空的，跳出明细的话主要是解析行为日志的浏览pageId，这里用到了CEP编程，我们日志数据里有一个字段是lastPageID，如果他为空的话证明是访问入口，所以我们定义一个严格连续的模式，匹配10秒内的两次访问都为入口的满足条件的模式，这样可以确定第一条一定为跳出行为，而第二条在侧输出流拿到，也是跳出行为，这样可以解决用户在短时间内多次访问入口然后立马退出的情况。

然后针对宽表涉及的话，就是用到了前面提到的读取维度数据，例如订单宽表，当来了事实数据之后我们去读取对应的维度信息，先将订单表和订单详情表两个事实表进行intervial join ，设定了10秒的时间间隔，对join结果进行补齐维度，去读相关的六个维度表的信息，例如商品3级分类，地区信息，sku信息，商品平台信息等，在这一层还有一个宽表的实现方法有点特殊就是支付宽表，因为下单和支付最多有半小时以上的间隔，所以用intervaljoin，用payment流interval join orderinfo流，指定一个-45分组的下界，

然后再dws层主要就是五张宽表，用户主题，商品注意，地区主题，搜索关键词主题，商品行为关键词主题宽表。然后这些宽表写入clickhouse，后续查询clickhouse进行一些指标的展示。我们主要是前端自己写了可视化组件进行展示，我也基于一些可视化组件进行过测试，例如tableau，superset，百度云的sugar powerbi等

